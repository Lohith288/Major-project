# -*- coding: utf-8 -*-
"""algoupdated2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Cc2-1qUGdrofWWMP19JkYCvMe99iZoPV

<a href="https://colab.research.google.com/github/Lohith288/project/blob/main/Untitled1.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>
"""

import numpy as np
import pandas as pd
from pathlib import Path
import matplotlib.pyplot as plt
import seaborn as sns
from skimage.io import imread
import cv2

from google.colab import drive
drive.mount('/content/drive')

"""***Getting the image path***"""

# initializing path
data_dir = Path('/content/drive/MyDrive/CT KIDNEY DATASET')
train_dir = data_dir

# Get the path to the normal and pneumonia sub-directories
Normal_Cases_dir = train_dir / 'Normal'
Cyst_Cases_dir = train_dir / 'Cyst'
Stone_Cases_dir = train_dir / 'Stone'
Tumor_Cases_dir = train_dir / 'Tumor'

# Getting the list of all the images
Normal_Cases = Normal_Cases_dir.glob('*.jpg')
Cyst_Cases = Cyst_Cases_dir.glob('*.jpg')
Stone_Cases = Stone_Cases_dir.glob('*.jpg')
Tumor_Cases = Tumor_Cases_dir.glob('*.jpg')

# An empty list for inserting data into this list in (image_path, Label) format
train_data = []

# Labeling the Cyst case as 0
for img in Cyst_Cases:
    train_data.append((img, 0))

# Labeling the Normal case as 1
for img in Normal_Cases:
    train_data.append((img, 1))

# Labeling the Stone case as 2
for img in Stone_Cases:
    train_data.append((img, 2))

# Labeling the Tumor case as 3
for img in Tumor_Cases:
    train_data.append((img, 3))

# Making a data frame using pandas (creating CSV file)
train_data = pd.DataFrame(train_data, columns=['image', 'label'], index=None)

# Select random data row from the dataframe and show and index it.
train_data = train_data.sample(frac=1.).reset_index(drop=True)
train_data.head()

"""***Returns all the unique values of Label in train_data***"""

train_data['label'].unique()

"""***Returns number of rows and columns***"""

train_data.shape

"""***Getting the count of each class (Normal, Cyst, Tumor, Stone)***"""

cases_count = train_data['label'].value_counts()
cases_count

"""***Plotting the Graph***"""

plt.figure(figsize = (8,6))  # Size of graph
sns.barplot(x = cases_count.index, y = cases_count.values)
plt.title('Number of Cases', fontsize=14)
plt.xlabel('Case Type', fontsize = 12)
plt.ylabel('Count', fontsize = 12)
plt.xticks(range(len(cases_count.index)),['Cyst(0)', 'Normal(1)','Stone(2)','Tumor(3)'])
plt.show()

"""***Getting few samples for both the classes***"""

# Gets 5 data from each cases
Cyst_Samples = (train_data[train_data['label'] == 0]['image'].iloc[:5]).tolist()
Normal_Samples = (train_data[train_data['label'] == 1]['image'].iloc[:5]).tolist()
Stone_Samples = (train_data[train_data['label'] == 2]['image'].iloc[:5]).tolist()
Tumor_Samples = (train_data[train_data['label'] == 3]['image'].iloc[:5]).tolist()

# Combining data in one variable
samples = Cyst_Samples + Normal_Samples + Stone_Samples + Tumor_Samples

del Cyst_Samples, Normal_Samples, Stone_Samples, Tumor_Samples

"""***Displaying the picture***"""

f, ax = plt.subplots(4, 5,figsize=(30,30)) # Initilizing the graph where image is to be display

for i in range(20):
    img = imread(samples[i]) # reading the image
    ax[i//5, i%5].imshow(img, cmap='gray') # displaying the image

    # putting title in the images
    if i<5:
        ax[i//5, i%5].set_title("Cyst_samples")
    elif i<10:
        ax[i//5, i%5].set_title("Normal_samples")
    elif i<15:
        ax[i//5, i%5].set_title("Stone_samples")
    elif i<20:
        ax[i//5, i%5].set_title("Tumor_samples")

    # removing the scales in the graph
    ax[i//5, i%5].axis('off')
    ax[i//5, i%5].set_aspect('auto')

plt.show()

"""***Get the path to the normal and pneumonia sub-directories***"""

Normal_Cases_dir = train_dir / 'Normal'
Cyst_Cases_dir = train_dir / 'Cyst'
Stone_Cases_dir = train_dir / 'Stone'
Tumor_Cases_dir = train_dir / 'Tumor'

"""***Getting the list of all the images***"""

Normal_Cases = Normal_Cases_dir.glob('*.jpg')
Cyst_Cases = Cyst_Cases_dir.glob('*.jpg')
Stone_Cases = Stone_Cases_dir.glob('*.jpg')
Tumor_Cases = Tumor_Cases_dir.glob('*.jpg')
train_data = []
train_labels = []

"""# ***Data pre processing***"""

for img in Cyst_Cases:
    img = cv2.imread(str(img)) # Loading image
    img = cv2.resize(img, (28,28)) # resizing image
    if img.shape[2] == 1: # Number of channel in the dimension.
        img = np.dstack([img, img, img])
    img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB) # Changing image color
    img = np.array(img) # creating a numpy array
    img = img/255  # Normalization
    label = 'Cyst'
    train_data.append(img)
    train_labels.append(label)

for img in Normal_Cases:
    img = cv2.imread(str(img))
    img = cv2.resize(img,(28,28))
    if img.shape[2] == 1:
        img = np.dstack([img, img, img])
    img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)
    img = np.array(img)
    img = img/255
    label = 'Normal'
    train_data.append(img)
    train_labels.append(label)

for img in Stone_Cases:
    img = cv2.imread(str(img))
    img = cv2.resize(img, (28,28))
    if img.shape[2] == 1:
        img = np.dstack([img,img,img])
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    img = np.array(img)
    img = img/255
    label = "Stone"
    train_data.append(img)
    train_labels.append(label)

for img in Tumor_Cases:
    img = cv2.imread(str(img))
    img = cv2.resize(img, (28,28))
    if img.shape[2] == 1:
        img = np.dstack([img, img, img])
    img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)
    img = np.array(img)
    img = img/255
    label = "Tumor"
    train_data.append(img)
    train_labels.append(label)

train_data1 = np.array(train_data) # train_data or train_data1 is a 4D array from 3D image Array
train_labels1 = np.array(train_labels)
print('Total number of validation example: ', train_data1.shape)
print('Total number of Labels : ', train_labels1.shape)

train_data1[1]

train_labels1 = pd.DataFrame(train_labels1, columns = ['label'],index=None)
train_labels1

train_labels1['label'].unique()

train_labels1['label'] = train_labels1['label'].map({'Cyst':0,'Normal':1,'Stone':2,'Tumor':3})

train_labels1

print(train_data1.shape)
print(train_labels1.shape)

train_labels1.isnull().sum() # Checking for the missing(NULL) value in the labels1

"""# ***Image data set Imbalance***"""

from imblearn.over_sampling import SMOTE
smote = SMOTE() # Initilizing The SMOTE class
train_rows = len(train_data1) # getting total number or rows
train_data1 = train_data1.reshape(train_rows,-1)  # Converting 4D array to 2D Array
train_data2, train_labels2 = smote.fit_resample(train_data1,train_labels1) # Balancing Image Dataset using SMOTE

cases_count1 = train_labels2['label'].value_counts() # Counting values of diffrent image

#Plotting Graph for Label values
plt.figure(figsize=(8,6)) # Setting size of graph
sns.barplot(x=cases_count1.index, y=cases_count1.values)
plt.title('Number of cases', fontsize = 14)
plt.xlabel('Case Type',fontsize = 12)
plt.ylabel('Count', fontsize = 12)
plt.xticks(range(len(cases_count1.index)), ['Cyst(0)', 'Normal(1)', 'Stone(2)', 'Tumor(3)'])
plt.show()

train_data2.shape

train_data2 = train_data2.reshape(-1,28,28,3) # Converting 2D array to 4D Array
train_data2.shape

train_data

"""***Splitting the Dataset For training and testing and Validating using sklearn : train_test_split***"""

from sklearn.model_selection import train_test_split

# Splitting the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(train_data2, train_labels2, test_size=0.20, random_state=42)

# Splitting the training set further into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.10, random_state=42)

total_samples = len(train_data2)
train_percentage = len(X_train) / total_samples * 100
test_percentage = len(X_test) / total_samples * 100
val_percentage = len(X_val) / total_samples * 100

print(f"Train set percentage: {train_percentage:.2f}%")
print(f"Test set percentage: {test_percentage:.2f}%")
print(f"Validation set percentage: {val_percentage:.2f}%")

print("Training Data X : ",X_train.shape)
print("Testing Data X : ",X_test.shape)
print("Training Data y : ",y_train.shape)
print("Testing Data y : ",y_test.shape)
print("Validating Data X : ",X_val.shape)
print("Validating Data y",y_val.shape)
print("Image Size : ",X_train[0].shape)

"""# ***SUPPORT VECTOR MACHINE***"""

from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef

# Assuming X_train, y_train, X_test, y_test are already defined

# Reshape y_train and y_test using ravel()
y_train = np.ravel(y_train)
y_test = np.ravel(y_test)

# Create an SVM classifier
svm_classifier = SVC(kernel='linear', C=1.0)

# Fit the classifier to the training data
svm_classifier.fit(X_train, y_train)

# Make predictions on the test data
y_pred = svm_classifier.predict(X_test)

# Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:")
print(conf_matrix)

# Accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy * 100)

# Sensitivity (Recall)
sensitivity = recall_score(y_test, y_pred, average='micro')
print("Sensitivity (Recall):", sensitivity)

# Specificity
specificity = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])
print("Specificity:", specificity)

# Precision
precision = precision_score(y_test, y_pred,average='micro')
print("Precision:", precision)

# Negative Predictive Value
npv = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[1, 0])
print("Negative Predictive Value:", npv)

# False Positive Rate
fpr = 1 - specificity
print("False Positive Rate:", fpr)

# False Discovery Rate
fdr = 1 - precision
print("False Discovery Rate:", fdr)

# False Negative Rate
fnr = 1 - sensitivity
print("False Negative Rate:", fnr)

# F1 Score
f1 = f1_score(y_test, y_pred,average='micro')
print("F1 Score:", f1)

# Matthews Correlation Coefficient
mcc = matthews_corrcoef(y_test, y_pred)
print("Matthews Correlation Coefficient:", mcc)

"""# ***KNN***"""

import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef

# Assume you have a function to load your image data and labels
def load_data():
    # Load the data
    X_train_val, X_test, y_train_val, y_test = train_test_split(train_data2, train_labels2, test_size=0.20, random_state=42)

    # Split the rest into training and validation sets
    X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.10, random_state=42)

    return X_train, X_val, X_test, y_train, y_val, y_test

# Load the data
X_train, X_val, X_test, y_train, y_val, y_test = load_data()

# Convert y_train, y_val, and y_test to 1D arrays before splitting
y_train = np.ravel(y_train)
y_val = np.ravel(y_val)
y_test = np.ravel(y_test)

# Create a k-NN classifier
knn = KNeighborsClassifier(n_neighbors=3)

# Train the classifier on the training set
knn.fit(X_train, y_train)

# Evaluate the classifier on the test set
y_pred_test = knn.predict(X_test)


# Confusion Matrix
cm = confusion_matrix(y_test, y_pred_test)
print("Confusion Matrix:\n", cm)

# Accuracy
accuracy = accuracy_score(y_test, y_pred_test)
print("Accuracy: {:.2f}%".format(accuracy * 100))

# Sensitivity (Recall)
sensitivity = recall_score(y_test, y_pred_test,average='micro')
print("Sensitivity (Recall):", sensitivity)

# Specificity
specificity = cm[0, 0] / (cm[0, 0] + cm[0, 1])
print("Specificity:", specificity)

# Precision
precision = precision_score(y_test, y_pred_test,average='micro')
print("Precision:", precision)

# Negative Predictive Value
npv = cm[0, 0] / (cm[0, 0] + cm[1, 0])
print("Negative Predictive Value:", npv)

# False Positive Rate
fpr = 1 - specificity
print("False Positive Rate:", fpr)

# False Discovery Rate
fdr = 1 - precision
print("False Discovery Rate:", fdr)

# False Negative Rate
fnr = 1 - sensitivity
print("False Negative Rate:", fnr)

# F1 Score
f1 = f1_score(y_test, y_pred_test,average='micro')
print("F1 Score:", f1)

# Matthews Correlation Coefficient
mcc = matthews_corrcoef(y_test, y_pred_test)
print("Matthews Correlation Coefficient:", mcc)

"""# ***Random forest classifier***"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef

# Split the data into training, validation, and test sets
X_train, X_test, y_train, y_test = train_test_split(train_data2, train_labels2, test_size=0.20, random_state=42)
X_train, X_val, y_train, y_val = train_test_split(train_data2, train_labels2, test_size=0.10, random_state=42)

# Convert y_train, y_val, and y_test to 1D arrays using ravel()
y_train = y_train.to_numpy().ravel()
y_val = y_val.to_numpy().ravel()
y_test = y_test.to_numpy().ravel()

# Create a random forest classifier
rf = RandomForestClassifier(n_estimators=100)

# Train the classifier
rf.fit(X_train, y_train)

# Make predictions on the validation set
y_pred_val = rf.predict(X_val)

# Confusion Matrix
cm_val = confusion_matrix(y_val, y_pred_val)
print("Confusion Matrix (Validation):\n", cm_val)

# Accuracy
accuracy_val = accuracy_score(y_val, y_pred_val)
print("Accuracy (Validation): {:.2f}%".format(accuracy_val * 100))

# Sensitivity (Recall)
sensitivity_val = recall_score(y_val, y_pred_val,average='micro')
print("Sensitivity (Recall) (Validation):", sensitivity_val)

# Specificity
specificity_val = cm_val[0, 0] / (cm_val[0, 0] + cm_val[0, 1])
print("Specificity (Validation):", specificity_val)

# Precision
precision_val = precision_score(y_val, y_pred_val,average='micro')
print("Precision (Validation):", precision_val)

# Negative Predictive Value
npv_val = cm_val[0, 0] / (cm_val[0, 0] + cm_val[1, 0])
print("Negative Predictive Value (Validation):", npv_val)

# False Positive Rate
fpr_val = 1 - specificity_val
print("False Positive Rate (Validation):", fpr_val)

# False Discovery Rate
fdr_val = 1 - precision_val
print("False Discovery Rate (Validation):", fdr_val)

# False Negative Rate
fnr_val = 1 - sensitivity_val
print("False Negative Rate (Validation):", fnr_val)

# F1 Score
f1_val = f1_score(y_val, y_pred_val,average='micro')
print("F1 Score (Validation):", f1_val)

# Matthews Correlation Coefficient
mcc_val = matthews_corrcoef(y_val, y_pred_val)
print("Matthews Correlation Coefficient (Validation):", mcc_val)

"""# ***Decision Tree Classifier***"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef

# Assuming X_train, y_train, X_test, y_test are already defined

dt_classifier = DecisionTreeClassifier(random_state=42)
dt_classifier.fit(X_train, y_train)
y_pred_dt = dt_classifier.predict(X_test)

# Confusion Matrix
cm_dt = confusion_matrix(y_test, y_pred_dt)
tn_dt, fp_dt, fn_dt, tp_dt = cm_dt.ravel().reshape(4, 4)

# Accuracy
accuracy_dt = accuracy_score(y_test, y_pred_dt)

# Sensitivity (Recall)
sensitivity_dt = recall_score(y_test, y_pred_dt,average='micro')

# Specificity
specificity_dt = tn_dt / (tn_dt + fp_dt)

# Precision
precision_dt = precision_score(y_test, y_pred_dt,average='micro')

# Negative Predictive Value
npv_dt = tn_dt / (tn_dt + fn_dt)

# False Positive Rate
fpr_dt = fp_dt / (fp_dt + tn_dt)

# False Discovery Rate
fdr_dt = fp_dt / (fp_dt + tp_dt)

# False Negative Rate
fnr_dt = fn_dt / (fn_dt + tp_dt)

# F1 Score
f1_dt = f1_score(y_test, y_pred_dt,average='micro')

# Matthews Correlation Coefficient
mcc_dt = matthews_corrcoef(y_test, y_pred_dt)

print("Decision Tree Classifier:")
print("Confusion Matrix:\n", cm_dt)
print("Accuracy: {:.2f}%".format(accuracy_dt * 100))
print("Sensitivity (Recall): {:.2f}".format(sensitivity_dt))
print("Specificity: {:.2f}".format(specificity_dt[0]))
print("Precision: {:.2f}".format(precision_dt))
print("Negative Predictive Value: {:.2f}".format(npv_dt[0]))
print("False Positive Rate: {:.2f}".format(fpr_dt[0]))
print("False Discovery Rate: {:.2f}".format(fdr_dt[0]))
print("False Negative Rate: {:.2f}".format(fnr_dt[0]))
print("F1 Score: {:.2f}".format(f1_dt))
print("Matthews Correlation Coefficient: {:.2f}".format(mcc_dt))

"""# ***Linear Discriminant Analysis***"""

from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

# Assuming X_train, X_test, y_train, y_test are your training and testing data

# Create an LDA object
lda = LinearDiscriminantAnalysis()

# Fit the LDA to the training data
lda.fit(X_train, y_train)

# Predict the labels of the test data
y_pred = lda.predict(X_test)

# Calculate the confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)

# Calculate sensitivity (recall)
sensitivity = recall_score(y_test, y_pred,average='micro')

# Calculate specificity
specificity = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])

# Calculate precision
precision = precision_score(y_test, y_pred,average='micro')

# Calculate negative predictive value
npv = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[1, 0])

# Calculate false positive rate
fpr = conf_matrix[0, 1] / (conf_matrix[0, 1] + conf_matrix[0, 0])

# Calculate false discovery rate
fdr = conf_matrix[0, 1] / (conf_matrix[0, 1] + conf_matrix[1, 1])

# Calculate false negative rate
fnr = conf_matrix[1, 0] / (conf_matrix[1, 0] + conf_matrix[1, 1])

# Calculate F1 score
f1 = f1_score(y_test, y_pred,average='micro')

# Calculate Matthews correlation coefficient
mcc = matthews_corrcoef(y_test, y_pred)

# Print the results
print("Confusion Matrix:\n", conf_matrix)
print("Accuracy: {:.2f}%".format(accuracy * 100))
print("Sensitivity (Recall): {:.2f}".format(sensitivity))
print("Specificity: {:.2f}".format(specificity))
print("Precision: {:.2f}".format(precision))
print("Negative Predictive Value: {:.2f}".format(npv))
print("False Positive Rate: {:.2f}".format(fpr))
print("False Discovery Rate: {:.2f}".format(fdr))
print("False Negative Rate: {:.2f}".format(fnr))
print("F1 Score: {:.2f}".format(f1))
print("Matthews Correlation Coefficient: {:.2f}".format(mcc))

"""# ***XG BOOST***"""

import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef
from sklearn.preprocessing import LabelEncoder
import xgboost as xgb

# Function to calculate specificity from confusion matrix
def specificity(conf_matrix):
    return conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1] + conf_matrix[1, 0])

# Function to calculate negative predictive value from confusion matrix
def negative_predictive_value(conf_matrix):
    return conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])

# Function to calculate false positive rate from confusion matrix
def false_positive_rate(conf_matrix):
    return conf_matrix[0, 1] / (conf_matrix[0, 1] + conf_matrix[0, 0])

# Function to calculate false discovery rate from confusion matrix
def false_discovery_rate(conf_matrix):
    return conf_matrix[0, 1] / (conf_matrix[0, 1] + conf_matrix[1, 1])

# Function to calculate false negative rate from confusion matrix
def false_negative_rate(conf_matrix):
    return conf_matrix[1, 0] / (conf_matrix[1, 0] + conf_matrix[1, 1])

# Assuming X_train, X_test, y_train, y_test are your training and testing data

# Label encoding if needed
label_encoder = LabelEncoder()
y_train_encoded = label_encoder.fit_transform(y_train)
y_test_encoded = label_encoder.transform(y_test)

# XGBoost classifier
xgb_classifier = xgb.XGBClassifier(
    n_estimators=100,
    max_depth=3,
    learning_rate=0.1,
    objective='multi:softmax',
    num_class=len(np.unique(y_train_encoded)),
    random_state=42
)

xgb_classifier.fit(X_train, y_train_encoded)
y_pred_xgb = xgb_classifier.predict(X_test)

# Calculate the confusion matrix
conf_matrix_xgb = confusion_matrix(y_test_encoded, y_pred_xgb)

# Calculate accuracy
accuracy_xgb = accuracy_score(y_test_encoded, y_pred_xgb)

# Calculate sensitivity (recall)
sensitivity_xgb = recall_score(y_test_encoded, y_pred_xgb, average='weighted')

# Calculate specificity
specificity_xgb = specificity(conf_matrix_xgb)

# Calculate precision
precision_xgb = precision_score(y_test_encoded, y_pred_xgb, average='weighted')

# Calculate negative predictive value
npv_xgb = negative_predictive_value(conf_matrix_xgb)

# Calculate false positive rate
fpr_xgb = false_positive_rate(conf_matrix_xgb)

# Calculate false discovery rate
fdr_xgb = false_discovery_rate(conf_matrix_xgb)

# Calculate false negative rate
fnr_xgb = false_negative_rate(conf_matrix_xgb)

# Calculate F1 score
f1_xgb = f1_score(y_test_encoded, y_pred_xgb, average='weighted')

# Calculate Matthews correlation coefficient
mcc_xgb = matthews_corrcoef(y_test_encoded, y_pred_xgb)

# Print the results
print("Gradient Boosting with XGBoost:")
print("Confusion Matrix:\n", conf_matrix_xgb)
print("Accuracy: {:.2f}%".format(accuracy_xgb * 100))
print("Sensitivity (Recall): {:.2f}".format(sensitivity_xgb))
print("Specificity: {:.2f}".format(specificity_xgb))
print("Precision: {:.2f}".format(precision_xgb))
print("Negative Predictive Value: {:.2f}".format(npv_xgb))
print("False Positive Rate: {:.2f}".format(fpr_xgb))
print("False Discovery Rate: {:.2f}".format(fdr_xgb))
print("False Negative Rate: {:.2f}".format(fnr_xgb))
print("F1 Score: {:.2f}".format(f1_xgb))
print("Matthews Correlation Coefficient: {:.2f}".format(mcc_xgb))

"""# ***Logistic Regression***"""

import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef
from sklearn.metrics import precision_recall_fscore_support
from sklearn.preprocessing import StandardScaler

# Assume you have a function to load your image data and labels
def load_data():
    # Load the data
    X_train_val, X_test, y_train_val, y_test = train_test_split(train_data2, train_labels2, test_size=0.20, random_state=42)

    # Split the rest into training and validation sets
    X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.10, random_state=42)

    return X_train, X_val, X_test, y_train, y_val, y_test

# Load the data
X_train, X_val, X_test, y_train, y_val, y_test = load_data()

# Convert y_train, y_val, and y_test to 1D arrays before splitting
y_train = np.ravel(y_train)
y_val = np.ravel(y_val)
y_test = np.ravel(y_test)

# Feature Scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)

# Create a logistic regression model with adjustments
logreg = LogisticRegression(max_iter=2000, solver='saga', tol=1e-4, warm_start=True)

# Train the model on the scaled training set
logreg.fit(X_train_scaled, y_train)

# Evaluate the model on the scaled test set
y_pred_test = logreg.predict(X_test_scaled)

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred_test)
print("Confusion Matrix:\n", cm)

# Accuracy
accuracy = accuracy_score(y_test, y_pred_test)
print("Accuracy: {:.2f}%".format(accuracy * 100))

# Sensitivity (Recall)
sensitivity = recall_score(y_test, y_pred_test, average='micro')
print("Sensitivity (Recall):", sensitivity)

# Precision
precision = precision_score(y_test, y_pred_test, average='micro')
print("Precision:", precision)

# F1 Score
f1 = f1_score(y_test, y_pred_test, average='micro')
print("F1 Score:", f1)

# Matthews Correlation Coefficient
mcc = matthews_corrcoef(y_test, y_pred_test)
print("Matthews Correlation Coefficient:", mcc)

# Additional Metrics

# Negative Predictive Value (NPV)
npv = cm[0, 0] / (cm[0, 0] + cm[1, 0]) if (cm[0, 0] + cm[1, 0]) != 0 else 0
print("Negative Predictive Value (NPV):", npv)

# False Positive Rate (FPR)
fpr = cm[0, 1] / (cm[0, 1] + cm[1, 1]) if (cm[0, 1] + cm[1, 1]) != 0 else 0
print("False Positive Rate (FPR):", fpr)

# False Discovery Rate (FDR)
fdr = cm[0, 1] / (cm[0, 1] + cm[0, 0]) if (cm[0, 1] + cm[0, 0]) != 0 else 0
print("False Discovery Rate (FDR):", fdr)

# False Negative Rate (FNR)
fnr = cm[1, 0] / (cm[1, 0] + cm[1, 1]) if (cm[1, 0] + cm[1, 1]) != 0 else 0
print("False Negative Rate (FNR):", fnr)

import numpy as np
from sklearn.neighbors import KNeighborsClassifier
import cv2

# Load the trained KNN classifier
knn = KNeighborsClassifier(n_neighbors=5)  # Assuming the classifier is already trained and saved
knn.fit(X_train_flat, y_train)
# Load and preprocess the image for testing
image_path = '/content/drive/MyDrive/CT KIDNEY DATASET/Normal/Normal- (1).jpg'  # Provide the path to the image you want to test
img = cv2.imread(image_path)
resized_img = cv2.resize(img, (28, 28))  # Resize the image to match the input size expected by the classifier
img_flat = resized_img.reshape(1, -1)  # Flatten the image into a 1D array

# Predict the class of the image
predicted_class = knn.predict(img_flat)

# Print or display the predicted class
if predicted_class == 0:
    print("The image corresponds to a cyst.")
elif predicted_class == 1:
    print("The image corresponds to a normal.")
elif predicted_class == 2:
    print("The image corresponds to a Stone.")
elif predicted_class == 3:
    print("The image corresponds to a Tumor.")
else:
    print("Invalid class prediction.")

"""The image corresponds to a normal."""

